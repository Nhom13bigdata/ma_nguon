# -*- coding: utf-8 -*-
"""TEST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10vNwv6XSuT-An5FW02oBFMwqhDkiHc_h
"""

# Cai dat thu vien pyspark
!pip install pyspark



# truy cap drive
from google.colab import drive
drive.mount('/content/drive')

from pyspark.sql import SparkSession

# Khởi tạo một SparkSession
spark = SparkSession.builder \
    .appName("DistributedQueryExample") \
    .getOrCreate()

# Đọc dữ liệu từ file CSV và tạo DataFrame
file_path = '/content/drive/MyDrive/Du_lieu/diamonds.csv'
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Đăng ký DataFrame như một bảng tạm thời để sử dụng với Spark SQL
df.createOrReplaceTempView("orders")
output = spark.sql("SELECT * FROM orders")
output.show() # show hết

# truy theo dieu kien ( phần này ô có thể làm tùy chọn nhé)
# SQL đơn giản: SELECT FROM WHERE
# result : Bảng dữ liệu
result2 = df.filter("amount > 120").select("id", "orderid", "amount") # filter: điều kiện, select: các cột muốn hiển thị

result3 = df.filter("amount < 500").select("id", "orderid", "amount").orderBy(df['amount'].desc()) # oderBy : sắp xếp theo thứ tự

result4 = df.filter("amount>100").select("id", "orderid", "amount").groupby("id", "orderid").agg({'amount':"sum"}) # groupby: nhóm các cột , agg là having trong SQL

result5 = result2.join(result3, on='id', how='inner') # kết nối 2 bảng với nhau

"""**LẬP BẢNG MÔ TẢ**"""

# thư viện thêm
from pyspark.sql.functions import *
from pyspark.sql.types import NumericType
from pyspark.sql import Row

# Lấy danh sách các cột số
numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]

# Tạo list để lưu trữ các Row
rows = []

for col in numeric_cols:
    # Tính toán thống kê cho từng cột
    kq = df.select(
        max(col).alias('max'),
        min(col).alias('min'),
        stddev(col).alias('std'),
        variance(col).alias('var'),
        percentile_approx(col, 0.25, 10000000).alias('Q1'),
        percentile_approx(col, 0.5).alias('Q2'),
        percentile_approx(col, 0.75).alias('Q3')
    ).collect()[0] # <--- Sửa lỗi ở đây

    # Chuyển đổi Row object thành dictionary
    kq_dict = kq.asDict()

    # Tạo Row cho từng cột
    rows.append(Row(feature='max', value=kq_dict['max'], column=col))
    rows.append(Row(feature='min', value=kq_dict['min'], column=col))
    rows.append(Row(feature='std', value=kq_dict['std'], column=col))
    rows.append(Row(feature='var', value=kq_dict['var'], column=col))
    rows.append(Row(feature='Q1', value=kq_dict['Q1'], column=col))
    rows.append(Row(feature='Q2', value=kq_dict['Q2'], column=col))
    rows.append(Row(feature='Q3', value=kq_dict['Q3'], column=col))



# Tạo DataFrame từ list các Row
summary_df = spark.createDataFrame(rows)

# Pivot bảng để có dạng mong muốn
summary_df = summary_df.groupBy("feature").pivot("column").agg({"value": "first"})

summary_df.show()

spark.stop()